[{"content":"üöÄ Welcome to My Work Adventure! Join me in a virtual workspace powered by WorkAdventure! Walk around, interact, and collaborate in a dynamic 2D environment. Whether you want to discuss data science, AI, or just say hi, step into my virtual office and let\u0026rsquo;s connect!\nüëâ Enter My WorkAdventure\n","date":"2024-05-14T00:00:00Z","image":"https://zinef.github.io/p/myworkadventure/wa_hu_3fd69cf6e08e86b5.png","permalink":"https://zinef.github.io/p/myworkadventure/","title":"Welcome to my Work Adventure"},{"content":"A century ago we invented formal ways to describe computations precisely because everyday language was too squishy. Now we‚Äôre steering powerful systems with‚Ä¶ everyday language. Is that progress or a regression?\nShort answer: it‚Äôs a step sideways and up. We didn‚Äôt abandon formalism, we pushed it down a layer. Natural language is becoming the interface, while the machinery underneath is getting more formal (APIs, schemas, tools, verifiers). Below I‚Äôll unpack why the algorithmic formalists did what they did, how LLMs change the surface of programming, and when ‚ÄúEnglish as glue‚Äù is a good idea or a trap.\nWhy formalize algorithms in the first place ? Early 20th-century logicians tried to pin down what it means to compute at all. David Hilbert‚Äôs program asked whether there was a mechanical procedure to decide the truth of any statement in first-order logic (the Entscheidungsproblem). Alonzo Church (lambda calculus) and Alan Turing (Turing machines) independently answered no, but their negative answers gave us something priceless: precise models of effective procedure, what we now call algorithms. Their work, together with Post and Kleene, forms the backbone of the Church - Turing thesis and computability theory.\nFrom there, computer science doubled down on precision inside programming, not just in computability theory:\nSyntax got formal grammars (e.g., Backus‚ÄìNaur Form) to define languages unambiguously, famously in the ALGOL 60 report. Semantics and correctness got axioms and proofs (e.g., Hoare logic), and structured programming discouraged ambiguous control flow. The point was never to avoid natural language in discourse, but to ensure that what the machine sees is crisp, verifiable, and composable.\nSo why are we ‚Äúback‚Äù to natural language now? Because LLMs made natural language an effective specification medium for many tasks. A good prompt can compress years of prior work : data, code, papers ‚Ä¶ into a short instruction that selects and adapts knowledge. But crucially, recent LLM apps don‚Äôt stop at plain text:\nFunction/Tool calling turns a sentence into a typed API call with guaranteed structure. What the user says is fuzzy, what the system executes is precise. Agents + tools (search, code execution, calculators, retrievers) let models plan in language but act through formal interfaces. Structured outputs (JSON Schema) and constrained decoding force the model to emit data that matches a schema exactly, bridging natural prompts with machine-checkable results. In other words, we use natural language for intent capture and explanation, then immediately re-enter formal territory for execution and interoperability.\nAre we undoing the original reasons for formalism? No. We‚Äôre relocating formalism to the seams where it matters.\nAmbiguity control moved from the user‚Äôs prompt to the contracts at the boundary (schemas, types, tools). With structured outputs and grammar-constrained decoding, you can make the generator respect your formal spec while still letting users speak freely. Verification is increasingly externalized: the model proposes, specialized solvers/checkers verify. This reflects a broader trend: treat the LLM as a parser/planner and let formal tools enforce correctness downstream. Proven limits still apply. Church-Turing didn‚Äôt go away, undecidability and incompleteness still constrain what any program or model can guarantee. The practical response is to bound the problem and add safeguards, not to hope that prose resolves impossibility. Think of LLMs as stochastic compilers from intent to actions. We‚Äôre not programming in English, we‚Äôre using English to drive formal systems.\nWhen is natural language a good ‚Äúprogramming layer‚Äù? Great for:\nExploration \u0026amp; orchestration. ‚ÄúSummarize these docs, extract entities, then call the CRM API.‚Äù The natural language plan can be translated to tool calls with structured outputs so the rest of your stack stays typed. End-user customization. Users don‚Äôt want DSLs, they want results. NL prompts make power accessible, while your app constrains what can actually happen via tools and schemas. Risky for:\nHigh-assurance logic. Dijkstra warned in 1978 against ‚Äúnatural language programming‚Äù because ambiguity clashes with the precision that programs require. The warning is still relevant unless you couple NL with formal constraints and verification. A pragmatic pattern is: Prompt ‚Üí Plan ‚Üí Structured Calls ‚Üí Verify ‚Üí Persist. The prompt is the human-friendly layer, everything after is formal and testable.\nLLMs reuse existing knowledge, prompts just query it LLMs are trained on existing artifacts. At inference time they compose and contextualize that knowledge. Tool use and retrieval make this even clearer: the model plans in language but leans on external knowledge bases or APIs for facts and effects. Recent surveys of LLM-based agents document this emerging architecture, language for reasoning and coordination, tools for ground truth and action.\nBut didn‚Äôt people try ‚ÄúEnglish-like programming‚Äù before? Yes, COBOL‚Äôs Englishy syntax, HyperTalk, Inform 7, and decades of ‚Äúnatural language programming‚Äù experiments. The enduring critique, again from Dijkstra, is that surface readability doesn‚Äôt buy you semantic guarantees. What‚Äôs new now isn‚Äôt Englishy syntax, it‚Äôs learning-based intent capture + formal execution layers. The shift is closer to Andrej Karpathy‚Äôs ‚ÄúSoftware 2.0‚Äù idea, specifying behavior via data and learned models instead of only handwritten rules, now extended with a natural language interface on top.\nA sober take: is this a step up? Yes, with guardrails. We improved the human-computer interface without discarding the computer‚Äôs need for formality. The socio-technical win is accessibility and speed, the technical debt is that ambiguity and hallucination creep back in unless you constrain, ground, and verify. The state of the art is actively addressing this with structured outputs, grammar-constrained decoding, and agent frameworks that keep LLMs honest by delegating to formal tools.\nFurther reading \u0026amp; references Why formalize algorithms / historical roots. Stanford Encyclopedia entries on the Church-Turing Thesis and Computability, Turing (1936) and Church (1936) on the Entscheidungsproblem. plato.stanford.edu, dl.acm.org\nLanguage design and correctness. ALGOL 60 report (BNF for syntax), Hoare‚Äôs ‚ÄúAn Axiomatic Basis for Computer Programming‚Äù, Dijkstra‚Äôs ‚ÄúGo To Statement Considered Harmful.‚Äù mass:werk ‚Äì media environments,eli-project.sourceforge.net, homepages.cwi.nl\nNatural language programming debate. Dijkstra‚Äôs On the foolishness of ‚Äúnatural language programming‚Äù (1978/79), The New Yorker‚Äôs ‚ÄúWhat if natural language replaced programming?‚Äù for a modern cultural view. cs.utexas.edu\nLLMs as planners with tools. ReAct (reasoning+acting) and Toolformer (teaching models to use tools), surveys of LLM-based agents. arXiv, openai.com\nBridging NL to formal outputs. OpenAI structured outputs (JSON Schema adherence) and research on constrained/grammar-guided decoding. openai.com, arXiv, aclanthology.org\n","date":"2025-08-19T00:00:00Z","image":"https://zinef.github.io/p/algorithms-to-prompts-llms/turing-m_hu_1271c063d58f8e7e.jpeg","permalink":"https://zinef.github.io/p/algorithms-to-prompts-llms/","title":"From Algorithms to Prompts: Did We Just Loop Back or Level Up?"},{"content":"The JavaScript ecosystem has evolved far beyond web development, and the AI revolution is no exception. With the rise of powerful LLMs and the growing need for intelligent applications, JavaScript developers now have robust tools to build Retrieval-Augmented Generation (RAG) systems and AI agents directly in their preferred language.\nToday, libraries like LangChain.js and LlamaIndex.ts bring enterprise-grade AI capabilities to JavaScript, enabling developers to create sophisticated RAG systems and autonomous agents that can run in browsers, Node.js environments, or edge computing platforms.\nFor this implementation, we\u0026rsquo;ll be using locally deployed language models through Ollama, specifically lightweight models like tinyllama, llama3.2:1b ‚Ä¶ which provide a good performance for RAG and agent applications while running entirely on local hardware.\nIn this comprehensive guide, we\u0026rsquo;ll explore how to leverage LlamaIndex.ts to create both RAG systems and AI agents, complete with practical implementations and real-world use cases that demonstrate the framework\u0026rsquo;s capabilities.\nWhy JavaScript for AI Development? Before diving into implementation details, it\u0026rsquo;s worth understanding why JavaScript has become a compelling choice for AI development:\nUniversal Runtime: JavaScript runs everywhere - browsers, servers, mobile apps, and edge devices. This universality means your AI applications can be deployed across diverse environments without language barriers.\nReal-time Capabilities: JavaScript\u0026rsquo;s event-driven nature and WebSocket support make it ideal for building responsive AI applications that need to handle streaming responses and real-time interactions.\nEcosystem Maturity: With npm\u0026rsquo;s vast package ecosystem and mature tooling, JavaScript provides excellent developer experience and extensive third-party integrations.\nPerformance: Modern JavaScript engines like V8 offer impressive performance, and tools like WebAssembly bridge the gap for computationally intensive tasks.\nWhy LlamaIndex.ts for RAG and Agents? LlamaIndex.ts represents a paradigm shift for web developers entering the AI space. Unlike Python-centric alternatives, it integrates seamlessly with existing JavaScript infrastructures, enabling developers to build AI applications without context switching between languages.\nThe framework offers several compelling advantages:\nNative JavaScript Integration: Deploy RAG systems and agents directly within Node.js applications, Next.js projects, or even browser environments without complex language interoperability layers.\nPerformance Optimization: Built with modern JavaScript practices, LlamaIndex.ts leverages async/await patterns and streaming capabilities for responsive AI applications.\nEcosystem Compatibility: Integrates naturally with popular JavaScript libraries, databases, and web frameworks, reducing friction in existing development workflows.\nEdge Computing Ready: The lightweight nature of JavaScript makes it ideal for edge deployments, bringing AI capabilities closer to users.\nUnderstanding RAG Architecture Retrieval-Augmented Generation combines the power of large language models with external knowledge sources. Instead of relying solely on pre-trained knowledge, RAG systems retrieve relevant information from custom datasets and use it to generate more accurate, contextually relevant responses.\nThe typical RAG pipeline consists of four key stages:\nDocument Ingestion: Processing and preparing source documents Embedding Generation: Converting text into vector representations Vector Storage: Storing embeddings in a searchable format Retrieval and Generation: Finding relevant context and generating responses LlamaIndex.ts streamlines this entire pipeline while providing fine-grained control over each component.\nPrerequisites Node.js ‚â• 18 (‚â• 20 recommended) and npm\nOllama running locally (ollama serve)\nLocal models already pulled, e.g.:\n1 2 3 # choose a model you have locally (examples) ollama pull tinyllama # or gpt-oss-20b, llama3.1:8b, mistral, etc. ollama pull nomic-embed-text # embedding model A new empty folder for each project (we‚Äôll keep RAG and Agent separate).\nNote 1 : We‚Äôll use plain JavaScript (ESM) to keep things frictionless. If you prefer TypeScript, just rename files to .ts and add a tsconfig.json , the imports stay the same.\nNote 2 : The following code is intentionally kept simple and minimal so it‚Äôs easy to follow in a blog format. In a real project, you‚Äôd usually want to refactor it into separate modules (for example, moving the agent or RAG logic into its own file and keeping the CLI entrypoint clean). This also makes it easier to add more functionality later (extra tools, different LLMs/embeddings, richer error handling, etc.). The nice thing is that the core logic stays the same. You can scale the structure as your project grows.\nMinimal Local RAG Scaffold Create a folder : rag-llamaindex-js/:\n1 2 3 4 5 6 7 rag-llamaindex-js/ package.json index.js data/ your-notes.md another-file.txt storage/ # will be created automatically package.json\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;name\u0026#34;: \u0026#34;local-rag-llamaindex\u0026#34;, \u0026#34;private\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;module\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;node index.js\u0026#34;, \u0026#34;clean\u0026#34;: \u0026#34;rimraf storage\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;llamaindex\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;@llamaindex/ollama\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;@llamaindex/readers\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;rimraf\u0026#34;: \u0026#34;latest\u0026#34; } } Install:\n1 2 cd rag-llamaindex-js npm i Put a couple of small .md or .txt files into ./data .\nRAG 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 // Imports import { Settings, VectorStoreIndex, storageContextFromDefaults } from \u0026#34;llamaindex\u0026#34;; import { SimpleDirectoryReader } from \u0026#34;@llamaindex/readers/directory\u0026#34;; import { ollama, OllamaEmbedding } from \u0026#34;@llamaindex/ollama\u0026#34;; // Configure local LLM + local embeddings Settings.llm = ollama({ model: process.env.OLLAMA_LLM || \u0026#34;tinyllama\u0026#34; }); Settings.embedModel = new OllamaEmbedding({ model: process.env.OLLAMA_EMBED || \u0026#34;nomic-embed-text\u0026#34;, }); const DATA_DIR = \u0026#34;./data\u0026#34;; const PERSIST_DIR = \u0026#34;./storage\u0026#34;; // vector + docstore persisted locally // 1) Loading \u0026amp; ingestion async function loadDocuments() { const reader = new SimpleDirectoryReader(); return reader.loadData({ directoryPath: DATA_DIR }); } // 2) Indexing \u0026amp; embedding + 3) Storing (persisted via storageContext) async function buildIndex(documents) { const storageContext = await storageContextFromDefaults({ persistDir: PERSIST_DIR }); return VectorStoreIndex.fromDocuments(documents, { storageContext }); } // (Optional) If you want to reload later without re-embedding. // import { VectorStoreIndex } from \u0026#34;llamaindex\u0026#34;; // const storageContext = await storageContextFromDefaults({ persistDir: PERSIST_DIR }); // const index = await VectorStoreIndex.init({ storageContext }); // 4) Querying async function query(index, question) { const queryEngine = index.asQueryEngine({ similarityTopK: 3 }); const res = await queryEngine.query({ query: question }); return res; } // Entrypoint const question = process.argv.slice(2).join(\u0026#34; \u0026#34;) || \u0026#34;What are the key ideas in these documents?\u0026#34;; const docs = await loadDocuments(); const index = await buildIndex(docs); const res = await query(index, question); console.log(\u0026#34;\\nQ:\u0026#34;, question); console.log(\u0026#34;\\nAnswer:\\n\u0026#34;, res.response); console.log(\u0026#34;\\nSources:\u0026#34;); for (const s of res.sourceNodes ?? []) { console.log(\u0026#34;-\u0026#34;, s.node?.metadata?.file_name || s.id_); } Run it 1 2 3 4 5 # terminal 1 ollama serve # terminal 2 (in rag-llamaindex-js/) node index.js \u0026#34;Summarize the main points across these docs.\u0026#34; You‚Äôll see an answer and a short list of source files like this.\nMinimal Local Agent Scaffold Create a new folder : agent-llamaindex-js/ :\n1 2 3 agent-llamaindex-js/ package.json index.js package.json\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;name\u0026#34;: \u0026#34;local-agent-llamaindex\u0026#34;, \u0026#34;private\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;module\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;node index.js\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;llamaindex\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;@llamaindex/ollama\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;@llamaindex/workflow\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;zod\u0026#34;: \u0026#34;latest\u0026#34; } } Install:\n1 2 cd agent-llamaindex-js npm i Agent 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 // Imports import { Settings, tool } from \u0026#34;llamaindex\u0026#34;; import { agent } from \u0026#34;@llamaindex/workflow\u0026#34;; import { z } from \u0026#34;zod\u0026#34;; import { ollama } from \u0026#34;@llamaindex/ollama\u0026#34;; // Local LLM via Ollama Settings.llm = ollama({ model: process.env.OLLAMA_LLM || \u0026#34;llama3.2:1b\u0026#34; }); // A strict calculator tool const addTool = tool({ name: \u0026#34;sumNumbers\u0026#34;, description: \u0026#34;Add two numbers and return the sum as a string.\u0026#34;, parameters: z.object({ a: z.number().describe(\u0026#34;First addend\u0026#34;), b: z.number().describe(\u0026#34;Second addend\u0026#34;), }), execute: ({ a, b }) =\u0026gt; `${a + b}`, }); async function main() { const myAgent = agent({ systemPrompt: \u0026#34;You are a precise assistant. Use tools when helpful. After using tools, output only the final answer.\u0026#34;, tools: [addTool], }); const userInput = process.argv.slice(2).join(\u0026#34; \u0026#34;) || \u0026#34;Add 101 and 303\u0026#34;; const { data } = await myAgent.run(userInput); console.log(\u0026#34;\\nUser:\u0026#34;, userInput); console.log(\u0026#34;\\nAgent:\u0026#34;, data?.result ?? String(data)); } main().catch((e) =\u0026gt; { console.error(e); process.exit(1); }); Note : Ensure you have a model that supports tool using, here we‚Äôre using llama3.2:1b.\nRun it 1 2 3 4 5 6 # terminal 1 ollama serve # terminal 2 (in agent-llamaindex-js/) node index.js \u0026#34;Add 11 and 99\u0026#34; # Agent should call the tool and print: 110 You‚Äôll see an answer like this.\nProduction Considerations When deploying RAG systems and agents to production, several critical factors must be addressed:\nPerformance Optimization: Implement vector database caching, use connection pooling for database operations, and consider implementing response caching for frequently asked questions.\nSecurity and Privacy: Implement proper input sanitization, use environment-specific API keys, implement rate limiting, and ensure sensitive data is properly handled and not logged.\nMonitoring and Observability: Track query performance, monitor token usage and costs, implement error tracking, and maintain conversation quality metrics.\nScalability: Design for horizontal scaling, implement proper load balancing, consider edge deployments for reduced latency, and plan for data partitioning strategies.\nAdvanced Features and Extensibility LlamaIndex.ts supports numerous advanced features that can enhance your applications:\nCustom Vector Stores: Integration with specialized vector databases like Pinecone, Weaviate, or Qdrant for production-scale deployments.\nMulti-modal Capabilities: Support for processing images, audio, and other media types alongside text documents.\nFine-tuning Integration: Capability to work with custom fine-tuned models for domain-specific applications.\nReal-time Updates: Implement systems for updating knowledge bases in real-time as new information becomes available.\nReferences \u0026amp; Further Reading LlamaIndex.TS (JS/TS) docs Loading data LlamaIndex Workflows / agents Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks LlamaIndex RAG blog Make your own RAG - Huggingface blog Huggingface\u0026rsquo;s Agents Course ","date":"2025-08-16T00:00:00Z","image":"https://zinef.github.io/p/llamaindex-rag-agents-javascript/llamaindex_js_hu_28011077c203ac03.png","permalink":"https://zinef.github.io/p/llamaindex-rag-agents-javascript/","title":"Building RAG and AI Agents using JavaScript: A Practical Guide with LlamaIndex.ts"},{"content":"Today marks a significant shift in OpenAI\u0026rsquo;s approach to AI democratization. The company has released their first open-weight models: GPT-OSS-120B and GPT-OSS-20B, bringing state-of-the-art reasoning capabilities directly to developers and researchers worldwide.\nAfter years of keeping their most advanced models behind API walls, OpenAI is finally embracing the open-source movement that has been thriving with competitors like Meta, Mistral, and DeepSeek. This move isn\u0026rsquo;t just about catching up, it\u0026rsquo;s about making advanced AI accessible to anyone with the hardware to run it.\nWhat Are GPT-OSS Models? The GPT-OSS family consists of two models designed specifically for reasoning tasks:\nGPT-OSS-120B: A 117-billion parameter model optimized for complex reasoning GPT-OSS-20B: A smaller 21-billion parameter model for resource-constrained environments Both models use a mixture-of-experts (MoE) architecture with 4-bit quantization (MXFP4), enabling faster inference while maintaining performance. According to OpenAI, these models offer \u0026ldquo;state-of-the-art open-weights reasoning\u0026rdquo; with \u0026ldquo;strong real-world performance comparable to o4-mini\u0026rdquo;.\nTechnical Capabilities and Performance What sets these models apart is their focus on reasoning rather than general language modeling. OpenAI claims they outperform similarly sized models on reasoning through complex tasks, making them particularly suitable for applications requiring logical thinking and problem-solving.\nThe 4-bit quantization is particularly interesting from a practical standpoint. This compression technique allows the models to run efficiently on consumer hardware while maintaining most of their reasoning capabilities. Sam Altman noted that the smaller model can even run \u0026ldquo;on your phone\u0026rdquo;, though I\u0026rsquo;d be curious to see the actual performance metrics on mobile hardware.\nLocal Deployment Possibilities One of the most exciting aspects of these releases is the potential for local deployment. NVIDIA has already announced optimization for RTX GPUs, and reports suggest the models run well on Apple Silicon Macs.\nFor developers and researchers, this means:\nPrivacy: Sensitive data never leaves your infrastructure Customization: Full control over fine-tuning and adaptation Cost Control: No per-token API charges for high-volume applications Offline Operation: Models work without internet connectivity The hardware requirements will be significant for the larger model, but the 20B version should be accessible to many developers with modern workstations or cloud instances.\nStrategic Implications This release represents a notable strategy shift for OpenAI. The move positions them to compete directly with Meta, Mistral, and DeepSeek in the open-weight space, acknowledging that closed models alone aren\u0026rsquo;t sufficient to maintain market position.\nOpenAI frames this as part of their mission \u0026ldquo;to put AI in the hands of as many people as possible\u0026rdquo;, but it\u0026rsquo;s also a practical response to the growing success of open-source alternatives. The community has demonstrated that open models can match or exceed proprietary ones in many tasks, and OpenAI seems to be adapting to this reality.\nWhat This Means for Developers If you\u0026rsquo;ve been working exclusively with API-based models, GPT-OSS opens new possibilities:\nAgentic Applications: These reasoning models enable \u0026ldquo;agentic AI applications such as web search, in-depth research and many more\u0026rdquo;. The ability to run reasoning models locally could significantly improve the reliability and cost-effectiveness of AI agents.\nResearch and Experimentation: Having full access to model weights enables deeper research into reasoning mechanisms, potential security vulnerabilities, and novel applications that aren\u0026rsquo;t possible with API-only access.\nProduction Flexibility: Organizations can now deploy OpenAI-quality reasoning models in environments where API calls aren\u0026rsquo;t feasible or desirable.\nGetting Started The models are available through standard open-source channels, and the community has already begun integrating them into popular frameworks. NVIDIA\u0026rsquo;s RTX AI Garage provides optimized versions for their hardware, which should make deployment more straightforward for developers with compatible GPUs.\nFor those interested in experimenting, I\u0026rsquo;d recommend starting with the 20B model to understand the capabilities before investing in the hardware needed for the larger version.\nLooking Forward This release feels like a watershed moment for AI accessibility. While OpenAI continues developing frontier models behind closed doors, opening their reasoning models to the community will likely accelerate innovation in AI applications and research.\nThe combination of state-of-the-art reasoning capabilities with local deployment flexibility creates opportunities we haven\u0026rsquo;t had before. I\u0026rsquo;m particularly excited to see how the community adapts these models for specialized reasoning tasks and what novel applications emerge.\nAs someone who has worked extensively with both open and closed models, I believe this move will ultimately benefit everyone, pushing both open-source development and proprietary research forward through increased competition and collaboration.\nThe era of truly accessible, high-quality reasoning AI has begun. The question now is what we\u0026rsquo;ll build with it.\nReferences OpenAI\u0026rsquo;s Official GPT-OSS Announcement Hugging Face Blog: Welcome GPT OSS NVIDIA RTX AI Garage Integration OpenAI Open Models Page ","date":"2025-08-05T00:00:00Z","image":"https://zinef.github.io/p/openai-gpt-oss-models/open_ai_hu_f26f98c6c4b55d0c.jpg","permalink":"https://zinef.github.io/p/openai-gpt-oss-models/","title":"OpenAI Goes Open Source: Introducing GPT-OSS Models"},{"content":"I\u0026rsquo;ve recently been experimenting with an exciting project at the intersection of AI and 3D design: Blender-MCP, which connects Claude Sonnet or other LLMs to Blender\u0026rsquo;s UI through an MCP server.\nThe concept is fascinating - using natural language prompts to create 3D models without touching traditional modeling tools. After seeing impressive demos circulating on social media, I had to try it myself.\nWhat works beautifully Blender-MCP shines when working with basic geometric shapes. The system understands concepts like \u0026ldquo;create a sphere,\u0026rdquo; \u0026ldquo;add a cylinder,\u0026rdquo; and \u0026ldquo;position a cube\u0026rdquo; remarkably well. It can follow step-by-step instructions to build compositions of these simple elements into more complex structures.\nIn one experiment, I built a simple castle using progressive prompts, guiding the AI through the process of creating towers, walls, and architectural details. The results were promising - while not photorealistic, the AI understood the concept and executed a recognizable castle structure.\nCurrent limitations Like many cutting-edge tools, Blender-MCP is still evolving. Complex organic shapes or highly detailed models remain challenging. The system works best when you break down complex ideas into smaller, manageable steps using simple geometry as building blocks.\nI found the most success when taking an iterative approach - starting with basic shapes and gradually refining them through additional prompts, rather than expecting perfect results from a single detailed instruction.\nThe bigger picture What excites me most about Blender-MCP isn\u0026rsquo;t just what it can do today, but what it represents for the future of creative workflows. The project demonstrates how AI can lower barriers to 3D design, potentially making these tools accessible to those without traditional modeling expertise.\nThe open-source community has already begun enhancing the project with marketplaces for ready-to-use models and additional plugins, showing the power of collaborative innovation.\nCombining AI Tools for Superior Results What I\u0026rsquo;ve discovered is that the true power of AI-driven 3D design emerges when combining complementary tools. While Blender-MCP excels with geometric shapes and positioning, it currently struggles with complex organic forms. This is where pairing it with state-of-the-art ML models like Hunyuan3D-2 creates a workflow greater than the sum of its parts.\nHunyuan3D-2: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation deserves special recognition. This remarkable model can generate detailed 3D assets from simple 2D images with impressive fidelity. The results are surprisingly sophisticated for an AI-generated model, especially considering how new this technology is.\nIn a recent experiment, I tested this workflow with a figurine of Luffy. I first used Hunyuan3D-2 to generate the base 3D model from a 2D reference, then imported it into Blender. From there, I leveraged Blender-MCP\u0026rsquo;s natural language interface to enhance and smooth the results. The combined approach allowed me to achieve in minutes what would have taken hours of manual modeling.\nThis hybrid workflow represents what I believe is the future of AI-assisted creation - not replacing human creativity, but amplifying it by handling technical barriers and time-consuming tasks. By strategically combining AI tools based on their strengths, designers can save significant time while achieving higher quality results.\nLooking forward As MCP (Model Context Protocol) technology continues to develop, we\u0026rsquo;ll likely see rapid improvements in these tools\u0026rsquo; capabilities. What requires multiple careful prompts today might be achieved with a single instruction tomorrow.\nIf you\u0026rsquo;re interested in exploring this space, I encourage you to check out the Blender-MCP project and Anthropic\u0026rsquo;s documentation on MCP. Even if you encounter limitations, contributing feedback helps advance these technologies.\nReferences Blender-mcp : https://github.com/ahujasid/blender-mcp Model Context Protocol : https://www.anthropic.com/news/model-context-protocol Hunyuan3D-2 github repo: https://github.com/Tencent/Hunyuan3D-2 Hunyuan3D-2 HF Spaces : https://huggingface.co/spaces/tencent/Hunyuan3D-2 ","date":"2025-03-20T00:00:00Z","image":"https://zinef.github.io/p/blender-mcp/blender_mcp_cover_hu_fc7164a5cff0eb8f.jpg","permalink":"https://zinef.github.io/p/blender-mcp/","title":"AI-Assisted 3D Design: Exploring Blender-MCP"},{"content":"Introduction As deep learning models become increasingly integrated into critical systems, the security implications of these AI models demand closer analysis. While much attention has focused on adversarial attacks and model poisoning, a more subtle threat lurks in the architecture of neural networks themselves: tensor steganography. This technique allows malicious actors to embed hidden payloads directly within model weights without significantly affecting performance, creating a potential vector for distributing malware that bypasses traditional security measures.\nIn this article, I\u0026rsquo;ll explore how tensor steganography works, demonstrate its feasibility in popular models like ResNet18, and explain why security professionals and ML engineers should be concerned about this emerging threat vector.\nWhat is Tensor Steganography? Steganography is the practice of hiding information within other non-secret data or a physical object to avoid detection. Unlike encryption, which makes data unreadable but visible, steganography conceals the very existence of the hidden data.\nTensor steganography applies this concept to neural networks by embedding data in the least significant bits of the floating-point values that make up model weights. These minor alterations are virtually undetectable through casual inspection and have minimal impact on model performance, making them an ideal hiding place for malicious code.\nFeasibility Analysis: The ResNet18 Case Study To understand the risk, let\u0026rsquo;s analyze the capacity for hidden data in a relatively small model like ResNet18. The largest convolutional layer in ResNet18 contains approximately 9.4MB of floating-point values . By manipulating just the least significant bits of each float\u0026rsquo;s mantissa, we can embed surprising amounts of data:\nBits Modified Per Float Storage Capacity 1-bit 294.9 kB 2-bits 589.8 kB 3-bits 884.7 kB 4-bits 1.2 MB 5-bits 1.5 MB 6-bits 1.8 MB 7-bits 2.1 MB 8-bits 2.4 MB This analysis reveals that even a modest model like ResNet18 can conceal up to 2.4MB of data by modifying just 8 bits per float in a single layer. Larger models commonly used in production environments could potentially hide much more ‚Äî up to 9MB of malicious code using only 3 bits per float in a single layer.\nImplementation: How Tensor Steganography Works Below is a Python implementation that demonstrates how to embed an arbitrary payload into a PyTorch model using steganography. This is a simple and naive example for illustrative purposes. In practice, a malicious actor could employ far more sophisticated techniques, making the detection and analysis of such hidden data significantly more challenging.\n1. Import Dependencies We first import the required libraries:\n1 2 3 4 5 6 7 import os import struct import hashlib from pathlib import Path import torch import numpy as np 2. Function Definition and Validations We define the function and validate the bit-depth parameter n:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def tensor_stego(model_path: Path, payload_path: Path, n: int = 1) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Embeds a payload inside the least significant bits (LSB) of the weights in a PyTorch model. Args: model_path (Path): Path to the PyTorch model (.pt or .pth). payload_path (Path): Path to the binary payload file. n (int): Number of LSBs to use (1-8). Default is 1. Returns: bool: True if embedding was successful, False otherwise. \u0026#34;\u0026#34;\u0026#34; if not (1 \u0026lt;= n \u0026lt;= 8): raise ValueError(\u0026#34;n must be between 1 and 8.\u0026#34;) 3. Load the Model We load the model :\n1 model = torch.load(model_path, map_location=torch.device(\u0026#34;cpu\u0026#34;)) 4. Read \u0026amp; Prepare the Payload Before embedding, we format the payload to include:\nFile size (so it can be reconstructed correctly) SHA-256 hash (for integrity verification) The actual payload data 1 2 3 4 5 6 7 8 with open(payload_path, \u0026#34;rb\u0026#34;) as file: payload_data = file.read() payload_size = os.path.getsize(payload_data) payload_hash = hashlib.sha256(payload_data).hexdigest().encode() # Pack size (4 bytes) + hash (64 bytes) + actual data payload = struct.pack(\u0026#34;i\u0026#34;, payload_size) + payload_hash + payload_data 5. Convert Payload to Bit Stream 1 2 3 4 5 6 7 bits = np.unpackbits(np.frombuffer(payload, dtype=np.uint8)) # Ensure the bit stream is a multiple of `n` by padding with zeros padding_size = (n - (len(bits) % n)) % n bits = np.pad(bits, (0, padding_size), constant_values=0) bits_iter = iter(bits) 6. Embed the Bits into the Model‚Äôs Tensors 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 for name, tensor in model.items(): tensor_data = tensor.data.numpy() # Check if the tensor has enough capacity to store the payload if tensor_data.size * n \u0026lt; len(bits): continue # Skip this tensor if it\u0026#39;s too small print(f\u0026#34;Embedding payload into layer: {name}\u0026#34;) # Compute the LSB mask mask = 0xff for i in range(0, tensor_data.itemsize): mask = (mask \u0026lt;\u0026lt; 8) | 0xff mask = mask - (1 \u0026lt;\u0026lt; n) + 1 # Create a read/write iterator for the tensor with np.nditer(tensor_data.view(np.uint32) , op_flags=[\u0026#34;readwrite\u0026#34;]) as tensor_iterator: # Iterate over float values in tensor for f in tensor_iterator: # Get next bits to embed from the payload lsb_value = 0 for i in range(0, n): try: lsb_value = (lsb_value \u0026lt;\u0026lt; 1) + next(bits_iter) except StopIteration: assert i == 0 # Save the model back to disk torch.save(model, f=model_path) return True # Embed the payload bits into the float f = np.bitwise_and(f, mask) f = np.bitwise_or(f, lsb_value) # Update the float value in the tensor tensor_iterator[0] = f return False 7. Save the Modified Model The model is automatically saved inside the loop when the entire payload is embedded. If embedding fails, we return False.\nN.B : This code includes a verification mechanism (SHA256 hash) to ensure the payload can be correctly extracted later. The payload format consists of the data size, a hash for verification, and the actual content.\nBelow is the full script in one place for convenience:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 import os import struct import hashlib from pathlib import Path import torch import numpy as np def tensor_stego(model_path: Path, payload_path: Path, n: int = 1) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Embeds a payload inside the least significant bits (LSB) of the weights in a PyTorch model. Args: model_path (Path): Path to the PyTorch model (.pt or .pth). payload_path (Path): Path to the binary payload file. n (int): Number of LSBs to use (1-8). Default is 1. Returns: bool: True if embedding was successful, False otherwise. \u0026#34;\u0026#34;\u0026#34; if not (1 \u0026lt;= n \u0026lt;= 8): raise ValueError(\u0026#34;n must be between 1 and 8.\u0026#34;) model = torch.load(model_path, map_location=torch.device(\u0026#34;cpu\u0026#34;)) with open(payload_path, \u0026#34;rb\u0026#34;) as file: payload_data = file.read() payload_size = os.path.getsize(payload_path) payload_hash = hashlib.sha256(payload_data).hexdigest().encode() # Pack size (4 bytes) + hash (64 bytes) + actual data payload = struct.pack(\u0026#34;i\u0026#34;, payload_size) + payload_hash + payload_data bits = np.unpackbits(np.frombuffer(payload, dtype=np.uint8)) # Ensure the bit stream is a multiple of `n` by padding with zeros padding_size = (n - (len(bits) % n)) % n bits = np.pad(bits, (0, padding_size), constant_values=0) bits_iter = iter(bits) for name, tensor in model.items(): tensor_data = tensor.data.numpy() # Check if the tensor has enough capacity to store the payload if tensor_data.size * n \u0026lt; len(bits): continue # Skip this tensor if it\u0026#39;s too small print(f\u0026#34;Embedding payload into layer: {name}\u0026#34;) # Compute the LSB mask mask = 0xff for i in range(0, tensor_data.itemsize): mask = (mask \u0026lt;\u0026lt; 8) | 0xff mask = mask - (1 \u0026lt;\u0026lt; n) + 1 # Create a read/write iterator for the tensor with np.nditer(tensor_data.view(np.uint32) , op_flags=[\u0026#34;readwrite\u0026#34;]) as tensor_iterator: # Iterate over float values in tensor for f in tensor_iterator: # Get next bits to embed from the payload lsb_value = 0 for i in range(0, n): try: lsb_value = (lsb_value \u0026lt;\u0026lt; 1) + next(bits_iter) except StopIteration: assert i == 0 # Save the model back to disk torch.save(model, f=model_path) return True # Embed the payload bits into the float f = np.bitwise_and(f, mask) f = np.bitwise_or(f, lsb_value) # Update the float value in the tensor tensor_iterator[0] = f return False Here\u0026rsquo;s a test snippet using ResNet18 to verify tensor_stego() :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import torchvision.models as models # Define paths model_path = Path(\u0026#34;resnet18.pth\u0026#34;) payload_path = Path(\u0026#34;payload.bin\u0026#34;) # Load a pretrained ResNet18 model and save its state_dict model = models.resnet18(pretrained=True) torch.save(model.state_dict(), model_path) # Generate a small binary payload with open(payload_path, \u0026#34;wb\u0026#34;) as f: f.write(b\u0026#34;The One Piece is real !\u0026#34;) # Example hidden message success = tensor_stego(model_path, payload_path, n=3) Here‚Äôs the reverse function to extract the hidden payload from the PyTorch model. This function will:\nLoad the model from the given path. Extract the LSBs of the weights to reconstruct the payload. Verify the extracted payload by checking its SHA-256 hash. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def extract_payload(model_path: Path, n: int = 1) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34; Extracts a hidden payload from the least significant bits (LSB) of the weights in a PyTorch model. Args: model_path (Path): Path to the PyTorch model (.pt or .pth). n (int): Number of LSBs used for embedding (1-8). Returns: bytes: Extracted payload if successful, None otherwise. \u0026#34;\u0026#34;\u0026#34; if not (1 \u0026lt;= n \u0026lt;= 8): raise ValueError(\u0026#34;n must be between 1 and 8.\u0026#34;) model = torch.load(model_path, map_location=torch.device(\u0026#34;cpu\u0026#34;)) extracted_bits = [] for name, tensor in model.items(): tensor_data = tensor.data.cpu().numpy() # Skip scalar tensors (0D) if tensor_data.ndim == 0: continue # Ensure data is in a format that supports bitwise operations tensor_data_flat = tensor_data.ravel().view(np.uint32) for f in tensor_data_flat: # Extract LSBs from the float lsb_value = f \u0026amp; ((1 \u0026lt;\u0026lt; n) - 1) extracted_bits.extend([(lsb_value \u0026gt;\u0026gt; i) \u0026amp; 1 for i in reversed(range(n))]) # Convert bitstream to bytes extracted_bytes = np.packbits(np.array(extracted_bits, dtype=np.uint8)).tobytes() # Extract payload size (first 4 bytes) payload_size = struct.unpack(\u0026#34;i\u0026#34;, extracted_bytes[:4])[0] payload_hash = extracted_bytes[4:68] extracted_payload = extracted_bytes[68:68 + payload_size] # Verify integrity computed_hash = hashlib.sha256(extracted_payload).hexdigest().encode() if computed_hash == payload_hash: print(\u0026#34;Payload successfully extracted and verified!\u0026#34;) return extracted_payload else: print(\u0026#34;Payload extraction failed: Hash mismatch.\u0026#34;) return None 1 2 3 4 extracted_payload = extract_payload(model_path=\u0026#34;resnet18.pth\u0026#34;, n=3) if extracted_payload: print(\u0026#34;Extracted Message:\u0026#34;, extracted_payload.decode(errors=\u0026#34;ignore\u0026#34;)) Security Implications The ability to hide executable code within model weights presents several concerning security implications:\nBypassing Security Scanning: Traditional malware detection tools don\u0026rsquo;t inspect ML model weights, allowing embedded malicious code to evade detection. Supply Chain Attacks: Pre-trained models downloaded from public repositories could contain hidden payloads that activate when the model is loaded. Persistent Backdoors: Since model weights are rarely inspected or modified after deployment, embedded code could remain undetected for extended periods. Execution Pathways: Concealing data within tensors is only the first step. The real threat emerges when this hidden payload is automatically extracted and executed, potentially exploiting vulnerabilities in how ML frameworks deserialize and handle model parameters. Prior research has demonstrated how adversaries can craft malicious models that trigger arbitrary code execution upon loading, bridging the gap between passive data hiding and active system compromise. For those interested in the practical exploitation of this vulnerability, this article provides a detailed breakdown of real-world attack scenarios, including a concrete example of how serialization flaws in PyTorch models can be leveraged for execution. Defensive Measures As a data professional responsible for model security, consider implementing these comprehensive protective measures:\nSource Verification: Only use models from trusted sources with verified signatures. Implement model signing as a means of performing integrity checking to detect tampering and corruption. Weight Analysis: Develop tools to analyze the distribution of least significant bits in model weights, which may reveal statistical anomalies indicating hidden data. Techniques such as entropy and Z-score analysis can help detect low-entropy payloads, though encrypted payloads remain challenging to identify. Sandboxed Loading: Load models in isolated environments with limited permissions to prevent potential code execution. This is especially critical when using pre-trained models downloaded from the internet, as current anti-malware solutions may not detect all code execution techniques. Security Scanning Tools: Utilize specialized tools like TrailOfBits\u0026rsquo; Fickling to detect simple attempts to exploit ML serialization formats. Monitor repositories like HuggingFace that have implemented security scanners for user-supplied models. Format Selection: Choose storage formats that offer enhanced security by avoiding data deserialization flaws, which are particularly vulnerable to exploitation. EDR Solutions: Deploy and properly tune Endpoint Detection and Response solutions to gain better insight into attacks as they occur, particularly for detecting advanced payloads delivered via ML models. Regular Security Audits: Conduct periodic security audits of your AI infrastructure, focusing on the integrity of deployed models and potential vulnerabilities in your ML pipeline. Conclusion Tensor steganography represents a sophisticated attack vector that could transform seemingly benign deep learning models into vehicles for malware distribution. As ML systems continue to proliferate across critical infrastructure, security professionals must expand their threat models to include these novel attack vectors.\nThe research demonstrates that even small models contain sufficient capacity to hide substantial malicious payloads with minimal impact on model performance. As larger models become standard, this capacity increases significantly‚Äîamplifying the potential threat.\nFor organizations developing or deploying ML systems, understanding and mitigating these risks should become an essential component of AI security protocols. The intersection of deep learning and cybersecurity continues to reveal new challenges that require vigilance and innovative defensive approaches.\nFor a more comprehensive understanding of this threat, I encourage readers to explore the references which provides detailed explanations of payload exploitation techniques, practical demonstrations, and extensive references. This in-depth resources offers security professionals the technical insights needed to develop robust defensive measures against this type of attacks.\nReferences : Weaponizing ML Models with Ransomware StegoNet: Turn Deep Neural Network into a Stegomalware EvilModel: Hiding Malware Inside of Neural Network Models Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch https://www.darkreading.com/application-security/hugging-face-ai-platform-100-malicious-code-execution-models https://medium.com/@limbagoa/securing-the-ai-supply-chain-051f8d43c5c4 https://www.techrepublic.com/article/pytorch-ml-compromised/ ","date":"2025-03-17T00:00:00Z","image":"https://zinef.github.io/p/tensorstego/tensor_stego_cover_hu_70b7b4215ab7e7e0.jpg","permalink":"https://zinef.github.io/p/tensorstego/","title":"The Hidden Threat: Using Steganography to Hide Malicious Payloads in Deep Learning Models"},{"content":"Introduction Brain tumors are among the most aggressive and lethal forms of cancer, and early detection is crucial for improving patient outcomes. However, analyzing medical images to diagnose and classify brain tumors presents several challenges due to the sheer size and complexity of whole-slide images (WSIs). In this article, I will share insights from my project, Converting Large Medical Images to Embeddings for Training Classifier Models, where I leveraged deep learning techniques to process high-resolution medical images efficiently. This is not just a technical breakdown but a real-world experience of tackling the problem, highlighting key lessons and takeaways.\nThe Challenge of Large-Scale Medical Image Analysis Medical images, particularly WSIs, are massive, often exceeding 100,000 pixels in resolution. Traditional image classification methods struggle to process such data due to memory constraints and computational complexity. My goal was to convert these high-dimensional images into meaningful embeddings that could be used for training classifier models to predict immune invasion stages in glioblastoma‚Äîa highly aggressive brain tumor.\nKey Challenges Data Size \u0026amp; Complexity: WSIs are gigapixel images that require efficient handling and storage. Annotation Scarcity: Unlike natural images, medical images require expert annotations, which are often limited. Feature Extraction: Extracting meaningful representations from these images without losing critical information. Computational Constraints: Training deep learning models on such large images is resource-intensive. The Solution: Transforming Images into Embeddings To address these challenges, I explored and adapted two state-of-the-art deep learning approaches:\nDeep Attention Multiple-Instance Survival Learning (DeepAttnMISL) DeepAttnMISL is a multiple-instance learning (MIL) approach designed for survival prediction from WSIs. Instead of classifying entire images at once, it breaks them into smaller regions (instances) and learns representations using an attention-based mechanism. Key steps included:\nPatch Extraction \u0026amp; Clustering: Extracting patches from WSIs and grouping them into phenotype clusters. Feature Extraction via CNNs: Using a pre-trained VGG model to generate feature embeddings for each patch. Attention-Based Pooling: Aggregating patch-level information using an attention-based MIL pooling layer to make patient-level predictions. Final Classification: Using the learned embeddings to train a classifier model to predict immune invasion stages (A, B, C, D). Vision Transformers (ViTs) Inspired by their success in NLP, I also explored Vision Transformers (ViTs), which process images as sequences of patches rather than relying on convolutions. ViTs leverage self-attention mechanisms to capture long-range dependencies, making them particularly suited for analyzing complex medical images.\nPatch Tokenization: Splitting the large image into smaller fixed-size patches. Embedding Generation: Encoding each patch into a vector representation. Self-Attention Mechanism: Learning relationships between different patches to detect patterns indicative of tumor presence. Classifier Training: Using the learned representations to train a predictive model. Key Takeaways The Importance of Representation Learning Converting images into embeddings significantly reduced the computational burden while preserving essential features. Choosing the right architecture for embedding extraction was crucial‚ÄîDeepAttnMISL provided structured phenotype-based representations, while ViTs captured global dependencies.\nAttention Mechanisms Enhance Interpretability Using attention-based pooling allowed us to identify the most critical regions of the WSIs, improving both accuracy and interpretability. This was particularly useful for medical experts who need to understand model predictions.\nPretrained Models Save Time \u0026amp; Resources Instead of training deep networks from scratch, leveraging pretrained models (e.g., VGG, ResNet) for feature extraction proved highly effective. Fine-tuning these models with domain-specific data further improved performance.\nComputational Constraints Are a Real Challenge Processing high-resolution WSIs required significant memory and GPU resources. Using techniques like patch extraction, dimensionality reduction, and efficient batching helped mitigate these challenges.\nConclusion Analyzing medical images for brain tumor detection is a complex but highly impactful challenge. By leveraging DeepAttnMISL and Vision Transformers, we can efficiently extract meaningful embeddings that improve classification accuracy while reducing computational costs. This project highlighted the power of attention mechanisms in deep learning and underscored the importance of adapting models to the unique constraints of medical imaging.\nFor those interested in deep learning applications in healthcare, this field offers vast opportunities to push the boundaries of AI-driven diagnostics. Whether you\u0026rsquo;re a researcher, practitioner, or enthusiast, the key takeaway is clear‚Äîsmart representation learning is the future of medical image analysis.\nWhat are your thoughts on AI in medical imaging? Have you worked on similar projects? Let\u0026rsquo;s discuss in the comments!\n","date":"2024-02-17T00:00:00Z","image":"https://zinef.github.io/p/lmianalysis/biospecimen-whole-slide-image-1_hu_4cb3674e7ba04551.png","permalink":"https://zinef.github.io/p/lmianalysis/","title":"How Can We Analyze Large Medical Images to Detect Brain Tumors? A Practical Guide"},{"content":"Introduction We hear more and more about MLOps. This practice, inspired by DevOps, aims to unify the tasks of developing Machine Learning applications with those of operations.\nIn this article, we will see what MLOps is, how it can be practiced and what tools are available to practice it.\nIn 2020, each person created at least 1.7 MB of data each second, according to techjury. That\u0026rsquo;s sound good for data scientists since there are so many theories and ideas to investigate, play with, and numerous findings and models to make.\nHowever, if we want to take this seriously and have these models interact with real-world business challenges and people, we must address the basics first, like acquiring and cleaning large amounts of data, setting up tracking and versioning for experiments and model training runs, setting up the deployment and monitoring pipelines for the models that do get to production.\nSimilar challenges occurred in the past when we tried to grow traditional software systems to accommodate additional users. DevOps provided a solution in the form of a set of practices for building, testing, deploying, and running large-scale software systems. DevOps shortened development times, enhanced deployment velocity, and made system releases auditable and durable.\nThat bring us to MLOps. It was formed at the junction of DevOps, Data Engineering, and Machine Learning, and while the concept is similar to DevOps, the implementation differs. ML systems are more experimental in nature, with additional components that are far more difficult to develop and run.\nWhat is MLOps? MLOps is the operationalization of Machine Learning model management. This aims to create an end-to-end process for creating, implementing and managing repeatable, testable and scalable machine learning models. MLOps aims to:\nUnify the machine learning delivery cycle and the application development cycle Automation of Machine Learning tests (Data Validation, model testing, model integration testing, etc) Enables the application of agile principles to the Machine Learning project Supports model creation in CI/CD Reduces the technical debt of ML models Must be independent of languages, framework, platform, etc. If there is anything to remember, it is that: MLOps is a set of practices that is intended to be as agile as possible and that must be based on the automation of the delivery and continuous integration processes of ML applications\nThe key phases of MLOps are:\nData gathering Data analysis Data transformation/preparation Model training \u0026amp; development Model validation Model serving Model monitoring Model re-training. DevOps \u0026amp; MLOps DevOps is a set of activities aimed at shortening the development life of a system and providing continuous delivery of high quality software. DevOps and MLOps both aim to bring software into a repeatable and fault tolerant workflow, but in MLOps that software also has a machine learning component.\nBefore deep diving into the comparison of DevOps and MLOps let\u0026rsquo;s recall what is the DevOps Cycle\nAs teams strive for a quicker code-build-deploy loop, DevOps is a crucial concept in practically all successful IT projects. This gives teams the ability to deploy new features more quickly, allowing them to complete projects faster and with a higher quality final result. However, without adequate DevOps methods, teams face manual work, inability to test, and, as a result, dangerous production deployments.\nAn ideal DevOps cycle will include the following five important pillars for a successful project (https://www.youtube.com/watch?v=uTEL8Ff1Zvk):\nReduce organizational silos Accept failure as normal Implement gradual changes Leverage tooling and automation Measure everything And the common DevOps cycle that includes all these pillars looks like this:\nComparison Cycle A code-validate-deploy cycle is included in both DevOps and MLOps pipelines. However, the MLOps pipeline also includes data and model stages that are necessary to create and train a machine learning model (see diagram below). As a result, MLOps has a few differences from traditional DevOps for each component of the workflow.\n\u0026ldquo;data\u0026rdquo; and \u0026ldquo;model\u0026rdquo; here represent, in most cases, the data labeling, data transformation or feature engineering and algorithm selection process which we can call the experiment phase.\nData labeling is the process of adding the target to a chunk of data records and the model will use this as a training set. In the case of a supervised ML model this type of data is critical.\nData transformation or feature engineering is also an important step to preprocess and prepare the most suitable structure for the ML model in order to produce good results.\nAnd selecting algorithm process depends on the nature of the prediction problem at hand.\nThe \u0026ldquo;Dev\u0026rdquo; and \u0026ldquo;Ops\u0026rdquo; parts are mostly the same at a high level.\nThe experimentation phase is unique to the data science lifecycle, which reflects how data scientists traditionally do their work. This differs from the way code developers do their work. The following diagram illustrates this life cycle in more detail.\nDevelopment and CI/CD The \u0026ldquo;development\u0026rdquo; takes two different meanings in each concept. In DevOps, by development we mean the code that creates an application or interface of some sort. The code is then wrapped up in an executable (artifact) that is deployed and validated against a series of tests. This cycle is ideally automated and continues until you have a final product. However, in MLOps the code is building/training a ML model. The output artifact here is a serialized file that can have data fed into it and produce inferences. The validation would be checking how well the trained model does against test data. Similarly, this is a cycle that continues until the model performs at a certain threshold.\nVersion control In a DevOps pipeline, version control is usually limited to tracking changes to code and artifacts. There are more things to track in an MLOps pipeline.\nAs mentioned before, the code in MLOps is building/training the ML model and it is an iterative cycle of experimenting. Each experimental run\u0026rsquo;s components and metrics must be tracked in order to appropriately recreate it afterwards for auditing reasons. The data set utilized in training (train/test split), the model construction code, and the model artifact are among these components. The hyper-parameters and model performance are among the metrics (e.g., error rate).\nWhen compared to standard software systems, this may appear to be a lot of information to keep track of. Fortunately, we have model registry tools (https://www.phdata.io/blog/what-is-a-model-registry/) as a tailor-made solution for versioning ML models.\nMonitoring Model drift is an additional component to monitor in MLOps, in addition to the application itself. Because data is continuously changing, your model must as well. Models trained on older data may or may not perform well on new data, particularly if the data is seasonal.\nIn order to keep your model up to date, it will need to be re-trained regularly (https://www.phdata.io/blog/when-to-retrain-machine-learning-models/) to gain consistent value from it.\nWhy MLOps? The importance of MLOps cannot be overstated. By establishing more efficient processes, utilizing data analytics for decision-making, and enhancing customer experience, machine learning helps individuals and enterprises deploy solutions that uncover previously untapped streams of revenue, save time, and decrease cost.\nThese objectives are difficult to achieve without a solid foundation to operate within. MLOps automates model creation and deployment, resulting in faster time to market and lower operating expenses. It assists managers and developers in making more strategic and agile decisions.\nMLOps provides as a road map for individuals, small teams, and even enterprises to fulfill their objectives regardless of their restrictions, such as sensitive data, limited resources, or a limited budget.\nBecause MLOps are not set in stone, you may choose the size of your map. You may try out various options and keep only what works for you.\nBest Practices In this section, we will see the best practices for different parts of an ML project, Team, DATA, Metrics\u0026amp;KPI, Model, Code and the Deployment.\nTeam Use A collaborative Development Platform: by making consistent use of a collaborative dev platform teams can work together more effectively. All this is possible because dev platforms provides easy access to data, code, information and other tools. One other interesting thing is that platforms help teams to work together asynchronously or remotely. Collaborative development environments include GitHub, GitLab, BitBucket, and Azure DevOps Server.\nWork Against a Shared Backlog: intent to avoid misunderstanding on the content, priority and status of tasks because an actively maintained backlog enables coordination of tasks within the team and with external stakeholders. It also helps in planning ahead and performing retrospective evaluations.\nCommunicate and Collaborate with others: The system that your team develops is meant to integrate with other systems within the context of a wider organization. this requires communication, alignment, and collaboration with others outside the team.\nData Use Sanity Checks for All External Data Sources: Avoid invalid or incomplete data being processed because data errors is crucial for model quality\nWrite Reusable Scripts for Data Cleaning and Merging: Avoid untidy data wrangling scripts, reuse code and increase reproducibility.\nEnsure Data Labelling is Performed in a Strictly Controlled Process: Avoid invalid or incomplete labels, Controlling the data labelling process ensures label quality \u0026ndash; an important quality driver for supervised learning algorithms.\nMake Data Sets Available on Shared Infrastructure (private or public): Avoid data duplication, data bottlenecks, or unnecessary transfer of large data sets.\nMetrics \u0026amp; KPI At first, track multiple metrics, not necessarily the best one: You want to make money, make your users happy, and make the world a better place. There are tons of metrics that you care about, and you should measure them all. However, early in the machine learning process, you will notice them all going up, even those that you do not directly optimize.\nEstablish Responsible AI Values: Explicitly align all stakeholders on the ethical values and constraints of your machine learning application\nEnforce Fairness and Privacy: Avoid irresponsible use of machine learning and decisions with negative societal impact.\nModel The first thing to do with the model is to get it simple, interpretable and get the infrastructure right, that what makes debugging easier\nShare a Clearly Defined Training Objective within the Team: Avoid misunderstandings between multi-disciplinary team members. In a multi-disciplinary team, members with different backgrounds may misinterpret training objectives. Therefore, it is important to clearly communicate the objectives within the team.\nCapture the Training Objective in a Metric that is Easy to Measure and Understand: Ensure the machine learning objective is easy to measure and it is a good proxy for the true objective.\nTest all Feature Extraction Code: Avoid bugs in the feature extraction code to ensure the non presence of errors and bugs in the whole process.\nEnable Parallel Training Experiments: Avoid deadlocks during experimentation. Machine learning relies heavily on empirical processes. In order to allow fast experimentation and avoid deadlocks, it is recommended to think upfront of experiment parallelisation.\nContinuously Measure Model Quality and Performance\nUse Versioning for Data, Model, Configurations and Training Scripts\nCode Run Automated Regression Tests: Avoid the introduction of bugs in code. When making changes, new defects can easily be introduced in existing code. A suite of automated regression tests helps to spot such defects as early as possible.\nUse Continuous Integration: Catch any code integration problems as early as possible. Code changes and additions may introduce problems into the software system as a whole. This can be detected by running an automated build script each time that code is committed to the versioning repository.\nAssure Application Security: to Prevent attackers from stealing or corrupting data, or from disrupting the availability of an application. Security incidents can lead to public data leaks, financial losses, or disrupt the availability of an application.\nDeployment Automate Model Deployment: Increase the ability to deploy models on demand, which increases availability and scalability. Deploying and orchestrating different components of an application can be a tedious task. Instead of manually packaging and delivering models, and in order to avoid manual interventions or errors, one can automate this task.\nEnable Shadow Deployment: Test a model\u0026rsquo;s behaviour on production data, without any impact on the service it provides. Before pushing a model into production, it is wise to test its quality and performance on data from production. In order to facilitate this task, one can deploy multiple models to \u0026lsquo;shadow\u0026rsquo; each other.\nContinuously Monitor the Behaviour of Deployed Models: To Avoid unintended behaviour in production models. Once a model is promoted to production, the team has to understand how it performs.\nLog Production Predictions with the Model\u0026rsquo;s Version and Input Data: To Enhance debugging, enable traceability, reproducibility, compliance and incident management. Tracing decisions back to the input data and the model\u0026rsquo;s version can be difficult. It is therefore recommended to log production predictions together with the model\u0026rsquo;s version and input data.\nMachine Learning Operations Maturity Model The purpose of this maturity model is to help clarify the principles and practices of Machine Learning Operations (MLOps). The maturity model shows continuous improvement in the creation and operation of a production-level machine learning application environment. You can use it as a metric to establish the incremental requirements needed to measure the maturity of a machine learning production environment and its associated processes.\nAs with most maturity models, the MLOps maturity model qualitatively assesses personas/culture, processes/structures, and objects/technologies. As the maturity level increases, the probability increases as incidents or errors lead to quality improvements in the development and production processes.\nThe MLOps maturity model encompasses five levels of technical capability:\nLevel Description Key points Technology 0 No MLOps - Difficulty in managing the full life cycle of machine learning models\n- Teams are heterogeneous and releases are painful\n- Most systems exist as \u0026ldquo;black boxes\u0026rdquo;, little feedback during/after deployment - Manual builds and deployments\n- Manual model and application testing\n- No centralized monitoring of model performance\n- Model training is manual 1 DevOps but no MLOps - Production releases are less painful than non-MLOps, but rely on the data team for each new model\n- Feedback on model performance in production is always limited\n- Difficult trace/reproduction results - Automated Builds\n- Automated tests for the application code 2 Automated Training - The training environment is fully managed and traceable\n- Easy to reproduce model\n- Versions are manual, but low friction - Automated model learning\n- Centralized tracking of training model performance\n- Model management 3 Automated Model Deployment - Low-friction, automatic releases\n- Full traceability from deployment to source data\n- Entire environment managed: training \u0026gt; testing \u0026gt; production - Integrated A/B testing of model performance for deployment\n- Automated testing for all code\n- Centralized tracking of training model performance 4 Full MLOps Automated Operations - Complete automated and easily monitored system\n- Production systems provide information on how to improve and, in some cases, automatically, new models\n- Approach of a system without dead time - Automated training and testing of models\n- Feedback, centralized metrics from a deployed model For more information on each level, please click on the link in the description.\nTypes of MLOps solutions Depending on your needs, the choice of the MLOps solution can be made on the following two types:\nEnd to end MLOps solution Custom build MLOps solution End to end MLOps solution These type of solution provides data scientists the ability to build, train and deploy ML models quickly, the solutions are fully managed services. And the best solutions for this type could be:\nAmazon:\nAmazon sagemaker: Amazon SageMaker is an ML service that enables data scientists and engineers, as well as MLOps engineers and business analysts, to create, train, and deploy ML models for any use case, regardless of their level of ML expertise. Google Cloud MLOps suite:\nDataflow: Fast, unified and cost-effective serverless batch and stream data processing. Kubeflow pipelines: Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers. Google Kubernetes Engine: A simple way to automatically deploy, scale and manage Kubernetes. TFX: TensorFlow Extended (TFX) is an end-to-end platform for deploying production ML pipelines Vertex AI Workbench: A single development environment for the entire data science workflow. ML kit: Machine learning for mobile developers, ML Kit brings Google\u0026rsquo;s machine learning expertise to mobile developers in a powerful and easy-to-use package. Make your iOS and Android apps more engaging, personalized, and helpful with solutions that are optimized to run on device. Microsoft Azure MLOps suite:\nAzure Machine Learning: Empower data scientists and developers to build, deploy, and manage high-quality models faster and with confidence. Accelerate time to value with industry-leading machine learning operations (MLOps), open-source interoperability, and integrated tools. Innovate on a secure, trusted platform designed for responsible AI applications in machine learning. Azure Kubernetes Service (AKS): Azure Kubernetes Service (AKS) offers the quickest way to start developing and deploying cloud-native apps, with built-in code-to-cloud pipelines and guardrails. Get unified management and governance for on-premises, edge, and multicloud Kubernetes clusters. Interoperate with Azure security, identity, cost management, and migration services. Azure Pipelines: Continuously build, test, and deploy to any platform and cloud. Azure Monitor: Azure Monitor helps you maximize the availability and performance of your applications and services. Custom built MLOps solution For making the pipeline robust, the custom-built solution is the best for you, this approach can help you avoid a single point of failure and makes your pipeline easier to audit, debug and more customizable. There are many tools available for this approach:\nProject jupyter: Free software, open standards, and web services for interactive computing across all programming languages Nbdev: a library that allows you to develop a python library in Jupyter Notebooks, putting all your code, tests and documentation in one place. Airflow: Airflow is a platform created to programmatically author, schedule and monitor workflows. Kubeflow: The Machine Learning Toolkit for Kubernetes. MLflow: An open source platform for the machine learning lifecycle Neptune: Neptune is an ML metadata store that was built for research and production teams that run many experiments. Optuna: An open source hyperparameter optimization framework to automate hyperparameter search Cortex: Deploy, manage, and scale machine learning models in production. Conclusion Now that you have in mind all the definitions and best practices of MLOps, you can chose one of the two solutions and go for it. Even better I invite you to consult my next article where I will present a solution and implementation of a MLOps pipeline.\nReferences https://se-ml.github.io/practices/ https://developers.google.com/machine-learning/guides/rules-of-ml https://docs.microsoft.com/en-ie/azure/architecture/example-scenario/mlops/mlops-maturity-model https://docs.microsoft.com/en-ie/azure/architecture/example-scenario/mlops/aml-decision-tree https://docs.microsoft.com/en-ie/azure/architecture/data-guide/azure-dataops-architecture-design https://docs.microsoft.com/fr-fr/azure/architecture/example-scenario/mlops/mlops-technical-paper https://neptune.ai/blog/category/mlops https://www.phdata.io/blog/when-to-retrain-machine-learning-models/ https://www.phdata.io/blog/what-is-a-model-registry/ https://www.youtube.com/watch?v=uTEL8Ff1Zvk https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#devops_versus_mlops https://www.datasciencecentral.com/mlops-vs-devops-the-similarities-and-differences/ https://towardsdatascience.com/building-a-devops-pipeline-for-machine-learning-and-ai-evaluating-sagemaker-cf7fdd3632e7 ","date":"2023-06-19T00:00:00Z","image":"https://zinef.github.io/p/mlops_6_23/MLOps-DevOps_hu_7b3ae877e8c164f3.webp","permalink":"https://zinef.github.io/p/mlops_6_23/","title":"MLOps: Introduction, Definitions and Best Practices"},{"content":"Have you ever struggled with the daunting task of code migration? Whether it‚Äôs moving from SAS to Python or Snowflake or BigQuery, C# to Python, PL/SQL to Snow SQL, or migrating ETL processes from Informatica to Snowflake, the challenges can be overwhelming. Not to mention the complexities involved in converting reports from Tableau to Power BI or Qlik to Power BI. However, with the groundbreaking capabilities of generative AI, these seemingly Herculean tasks can now be accomplished effortlessly and affordably. Say goodbye to traditional, time-consuming workflows and embrace the transformative power of generative AI in your migration endeavors.\nRecently, I embarked on a fascinating journey into the realm of Generative AI, and the discoveries I‚Äôve made are too exciting not to share. In this article, I have curated a collection of powerful use cases that demonstrate the remarkable potential of Generative AI to revolutionize the data science field. So, fasten your seatbelts as we explore the possibilities that lie ahead.\nUnderstanding Generative AI Before we claw into the captivating world of Generative AI, let‚Äôs take a moment to familiarize ourselves with its environment. Generative AI refers to a branch of artificial intelligence that focuses on creating models able of producing new and original content. These models can induce realistic images, vids, music, and text that nearly resembles human creations.\nGenerative AI can be divided into two subtypes, models that calculate the density (explicit density) and models that can only sample the density (implicit density), and these are further divided into subtypes, at the bottom of the tree we retrieve mostly these models :\nAutoencoders (VAE): Autoencoders serve as a fundamental building block of Generative AI. They are neural networks designed to learn efficient representations of data by encoding and decoding it. One compelling use case of autoencoders is their ability to facilitate fraud detection and anomaly detection.\nGenerative Adversarial Networks (GANs): GANs employ a two-part network comprising a generator and a discriminator, which work together to produce realistic outputs. GANs have found remarkable success in various domains, such as image synthesis, video generation, and even designing new products. For instance, GANs can aid in the creation of highly realistic images for marketing and advertising purposes or assist architects in envisioning realistic renderings of buildings that are yet to be constructed.\nDiffusion Models: Diffusion models are powerful generative AI techniques that excel in generating high-quality, realistic samples. These models learn the underlying probability distribution of a dataset and can then generate new samples that resemble the original data. They have shown great potential in fields such as image synthesis, medical anomaly detection, and data augmentation.\nLarge Language Models (LLMs): Among the various subtypes of Generative AI models, Large Language Models (LLMs) have been trending in recent years. These models are trained on vast amounts of text data and can generate coherent and contextually relevant text responses. With the advancements in LLMs, numerous applications have emerged, including language translation, chatbots, content generation, and even code completion. In this article, we will primarily focus on the intriguing use cases of LLMs.\nImage by LifeArchitect.ai/gpt-3\nImage by LifeArchitect.ai/gpt-4\nDeep Dive into Captivating Use Cases Now that we have laid the foundation, let‚Äôs dive into the captivating use cases that have caught my attention. These innovative applications of Generative AI are not only transforming industries but also enhancing the capabilities of data scientists, paving the way for groundbreaking advancements in their respective fields.\nCognitive Search using Embeddings In today\u0026rsquo;s data-driven world, the ability to extract relevant information quickly and efficiently is paramount. Traditional keyword-based search engines often fall short when it comes to understanding the context and nuances of user queries. However, with the advent of Generative AI, specifically the utilization of embeddings, a revolutionary approach to search has emerged.\nEmbeddings, in the context of Generative AI, refer to vector representations that capture the semantic meaning of words, phrases, or documents. By leveraging advanced techniques like GPT or BERT (Bidirectional Encoder Representations from Transformers), these embeddings enable machines to comprehend the underlying meaning and relationships within textual data.\nCognitive search powered by embeddings offers significant advantages over traditional search methods. It can understand the intent behind user queries, consider the context, and provide more accurate and relevant search results.\nHere\u0026rsquo;s an example to illustrate its potential: Imagine you are working in a large organization that generates vast amounts of data. You need to find specific documents related to a particular project. Instead of relying solely on keyword matching, cognitive search with embeddings can understand the project\u0026rsquo;s context and return documents that are semantically relevant. It can even suggest related documents that might be useful, even if they don\u0026rsquo;t contain the exact search terms.\nFurthermore, embeddings allow for semantic similarity searches. This means that you can search for documents that are conceptually similar to a given document, even if the keywords or phrases differ. For instance, if you have a document describing a marketing campaign, cognitive search can identify other documents discussing similar marketing strategies or related topics.\nBy harnessing the power of embeddings in cognitive search, organizations can enhance their knowledge discovery processes, improve search accuracy, and save valuable time and resources. Whether it\u0026rsquo;s within enterprise knowledge management systems, customer support portals, or e-commerce platforms, cognitive search using embeddings empowers users to explore and retrieve information more effectively.\nThis use case illustrates how Generative AI, particularly leveraging embeddings, revolutionizes the way we interact with and extract insights from textual data. The potential for cognitive search is vast, and as the technology advances, we can expect even more sophisticated applications that redefine how we access and make sense of information.\nAnything from Anything : Code Migration and Platform Conversion Generative AI has proven to be a game-changer when it comes to code migration and platform conversion. Traditional approaches to these tasks often involve significant time, effort, and potential risks. However, with the power of Generative AI, these processes can be streamlined, making them faster, more efficient, and cost-effective.\nCode migration : Migrating code from one language or platform to another can be a complex and time-consuming endeavor. Generative AI techniques, such as LLMs, can aid in this process by learning the underlying structure and patterns of the source code and generating equivalent code in the desired language or platform. For example, let‚Äôs consider the migration of code from SAS to Python. Instead of manually rewriting the entire codebase, Generative AI models can be trained on existing SAS code to learn the syntax, logic, and functionality. The models can then generate equivalent code in Python, reducing the effort and potential errors involved in the migration process. Similarly, migrating code from languages like C# to Python or PL/SQL to Snow SQL can be facilitated through Generative AI. By leveraging the power of AI, organizations can minimize the challenges associated with code migration, accelerate the transition process, and capitalize on the benefits of new technologies and platforms.\nPlatform Conversion : Platform conversion, such as migrating ETL processes from one platform to another, is another area where Generative AI shines. Take, for example, the migration from Informatica to Snowflake. Generative AI techniques can analyze the existing ETL workflows, understand the data transformations, and generate equivalent workflows in Snowflake‚Äôs ETL. This enables a seamless transition between platforms, ensuring data continuity and minimizing disruption. Moreover, Generative AI can assist in converting reports from one visualization platform to another. For instance, converting reports from Tableau to Power BI or Qlik to Power BI can be a laborious task. However, by leveraging the power of Generative AI, the models can learn the visualization structures, data mappings, and formatting styles of the source reports. They can then generate equivalent reports in the desired target platform, significantly reducing the time and effort required for manual conversion.\nAdditionally, Generative AI can be used in the automation of test cases. By training models on existing codebases and test suites, they can generate unit test cases for programming languages like Java or Python. This automation streamlines the testing process, enhances code quality, and frees up valuable time for data scientists to focus on higher-level tasks.\nGenerative AI‚Äôs ability to streamline code migration, platform conversion, report conversion, and test automation is transformative. It empowers organizations to adapt to new technologies, optimize workflows, and unlock efficiencies that were previously challenging to achieve. With Generative AI as a driving force, the data science field is witnessing a paradigm shift in how these tasks are approached and executed.\nBusiness Analytics Service Enablement Generative AI has the potential to transform business analytics services by enabling intuitive and user-friendly solutions. Imagine a scenario where clients can effortlessly extract insights from vast amounts of data, just like requesting information from a virtual assistant. Generative AI makes this a reality by empowering business analytics platforms, such as Power BI or Tableau, with advanced capabilities.\nClients often express the desire for a streamlined and automated analytics experience. They envision a solution where they can simply ask for specific data insights and receive visually appealing graphs or charts without the need for complex queries or manual data manipulation.\nGenerative AI brings this vision to life. By leveraging natural language processing and machine learning techniques, analytics platforms can understand and interpret user requests in plain language. Clients can say, ‚ÄúGive me the sales of this product in this region for this month‚Äù and the system will intelligently retrieve the relevant data, perform the necessary calculations, and present the requested insights in an intuitive and visually appealing manner.\nThe integration of Generative AI into business analytics services enables a self-service analytics experience that empowers users at all levels of an organization. Executives, business analysts, and data scientists alike can effortlessly explore and visualize complex data sets, extract valuable insights, and make data-driven decisions without the need for extensive technical knowledge or assistance.\nThis use case illustrates the transformative power of Generative AI in revolutionizing business analytics services. It not only enhances the accessibility of data but also empowers organizations to unlock the full potential of their data assets. By simplifying the analytics process, organizations can make faster, data-driven decisions, uncover hidden trends and patterns, and gain a competitive edge in their respective industries.\nAs Generative AI continues to advance, we can expect even more sophisticated analytics service enablement solutions. The ability to seamlessly interact with data and extract actionable insights will become increasingly intuitive, making data-driven decision-making a seamless part of everyday business operations.\nMedical NLP using BioGPT Medical professionals deal with a vast amount of unstructured textual data, including patient records, clinical notes, research papers, and more. Extracting valuable insights from this data can be a time-consuming and labor-intensive task. However, Generative AI, specifically Natural Language Processing (NLP) models like BioGPT, has emerged as a powerful tool to revolutionize medical data analysis and decision-making.\nBioGPT, a specialized variant of Generative Pre-trained Transformer (GPT) models, is specifically trained on medical literature and healthcare-related text. This pre-training equips it with a deep understanding of medical concepts, terminology, and context. By leveraging BioGPT, medical professionals and researchers can unlock valuable insights from vast amounts of unstructured medical data.\nOne key application of BioGPT is in clinical decision support. Medical professionals can input patient symptoms, medical history, and test results into the system, and BioGPT can analyze the data to provide recommendations for diagnosis, treatment options, and potential risk factors. This assists healthcare providers in making more informed decisions and improving patient outcomes.\nFurthermore, BioGPT can aid in biomedical research and literature review. By processing and analyzing a vast array of research papers, clinical trials, and scientific articles, BioGPT can identify relevant studies, extract key findings, and provide summaries or insights on specific medical topics. This significantly accelerates the research process, enabling scientists and clinicians to stay up-to-date with the latest advancements and make evidence-based decisions.\nAnother application of BioGPT in medical NLP is in coding and structuring medical records. Medical coding is a crucial process that ensures accurate reimbursement, clinical documentation, and data analysis. BioGPT can automatically extract relevant information from clinical notes and assign appropriate codes, simplifying the coding process and reducing the risk of errors.\nMoreover, BioGPT can assist in natural language understanding for electronic health records (EHRs). It can analyze and extract important clinical information from free-text EHRs, facilitating data mining and enabling population health analysis. This helps in identifying patterns, predicting disease outcomes, and improving healthcare delivery and planning.\nBy harnessing the power of BioGPT and medical NLP, healthcare professionals can streamline data analysis, enhance decision-making, and improve patient care. The combination of advanced language understanding and medical expertise empowers medical practitioners and researchers to extract valuable insights from vast amounts of unstructured medical data, revolutionizing the way healthcare is delivered and improving patient outcomes.\nThis use case showcases the tremendous potential of Generative AI, specifically BioGPT, in transforming medical NLP and revolutionizing healthcare data analysis, clinical decision-making, and biomedical research. As the technology continues to advance, we can expect even more sophisticated applications that redefine how medical data is analyzed, interpreted, and utilized in the pursuit of improved healthcare outcomes.\nAutomated Software Defect Closure Software defects are an inevitable part of the software development process, and closing them in a timely and efficient manner is crucial for delivering high-quality software. Generative AI offers a powerful solution to automate the process of identifying and closing software defects, saving time and resources for development teams.\nHere‚Äôs how automated software defect closure using Generative AI can benefit organizations:\nDefect Identification: Generative AI models can analyze codebases, logs, and error reports to identify patterns and anomalies that indicate the presence of software defects. By leveraging machine learning algorithms, these models can learn from historical data and develop a deep understanding of different types of defects. This enables them to accurately pinpoint potential issues within the codebase.\nAutomated Root Cause Analysis: Generative AI can assist in performing automated root cause analysis for software defects. By analyzing code changes, dependencies, and system logs, the models can determine the underlying cause of the defect. This information is valuable for developers, as it helps them understand the root cause and address it effectively.\nAutomated Bug Fix Generation: Once a software defect is identified and its root cause determined, Generative AI models can automatically generate potential bug fixes or suggest code changes to resolve the issue. These models leverage their understanding of programming languages, coding best practices, and defect patterns to generate high-quality fixes that adhere to coding standards.\nRegression Testing: After a bug fix is implemented, Generative AI can assist in performing automated regression testing. The models can generate test cases based on the defect and its fix, ensuring that the issue is resolved and that the fix does not introduce new defects or regressions in the software.\nContinuous Learning and Improvement: Generative AI models can continuously learn from the feedback provided by developers and testers. This feedback loop helps the models refine their bug detection and fix generation capabilities over time, leading to more accurate and effective defect closure.\nAutomated software defect closure using Generative AI streamlines the defect resolution process, accelerates bug fixes, and improves software quality. By automating repetitive and time-consuming tasks, development teams can focus on more critical aspects of software development, such as feature implementation and performance optimization.\nThis use case demonstrates the transformative potential of Generative AI in automating software defect closure. By leveraging machine learning and automation, organizations can improve software quality, shorten development cycles, and deliver more robust and reliable software products to their customers.\nSum up Generative AI, with its ability to understand, create, and automate, has emerged as a transformative force in the field of data science. Its applications, especially in the realms of Generative AI and Large Language Models (LLMs), have attracted clients and companies across various industries. While the adoption of Generative AI brings immense opportunities and advancements, it is crucial to acknowledge the continued importance of data scientists in harnessing the power of AI.\nThroughout this article, we have explored several captivating use cases that demonstrate the potential of Generative AI to revolutionize various domains. From cognitive search to business analytics service enablement, medical NLP, and automated software defect closure, Generative AI showcases its versatility in driving innovation and efficiency.\nBy leveraging Generative AI, organizations can meet the ever-increasing market demand and cater to their clients‚Äô needs more effectively. The ability to generate meaningful insights from vast amounts of data, automate complex tasks, and improve decision-making processes positions Generative AI as a valuable tool in today‚Äôs data-driven landscape.\nHowever, it is important to emphasize that Generative AI does not replace the role of data scientists and domain experts. Instead, it enhances their capabilities, enabling them to tackle more complex challenges and leverage AI-powered solutions effectively. Data scientists play a vital role in training, fine-tuning, and validating Generative AI models to ensure accuracy, ethical considerations, and alignment with business objectives.\nAs we look to the future, the potential of Generative AI continues to expand. The integration of advanced techniques, such as AutoEncoders, Generative Adversarial Networks (GANs), diffusion models, and Large Language Models (LLMs), opens up new possibilities for innovation and problem-solving.\nIn conclusion, Generative AI, with its diverse use cases and transformative capabilities, offers significant value to clients and companies in the data science field. By embracing Generative AI and recognizing the continued importance of data scientists, organizations can unlock the true potential of AI and drive impactful change in their respective industries.\nRemember, Generative AI is a powerful tool, but it is the collaboration between human expertise and AI capabilities that truly propels us towards a future where data-driven insights and solutions are readily accessible, efficient, and impactful.\nReferences Variational Autoencoders\nDeep Autoregressive Models\nDeep Generative Modelling\nAn Introduction to Generative Deep Learning\nBioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining\nThe GPT-3 Family: 50+ Models\nGPT-3.5 + ChatGPT: An illustrated overview\nGPT-4\nThe Illustrated Transformer\nDiffusion models\nInstructPix2Pix: Learning to Follow Image Editing Instructions\n","date":"2023-06-04T00:00:00Z","image":"https://zinef.github.io/p/genai_6_22/cover_hu_e95a4276bf860a84.jpg","permalink":"https://zinef.github.io/p/genai_6_22/","title":"Unleash Limitless Possibilities: Harness the Power of Generative AI to Meet Market Demands and Delight Your Clients"},{"content":"Introduction In the era of biological big data, multi-omics analysis represents a major challenge for researchers. How can we make sense of these gigantic biological datasets from different layers of cellular information? How can we integrate transcriptomic, proteomic, and metabolomic data to gain a comprehensive view of biological processes?\nThese are precisely the questions that my team and I attempted to answer through our project \u0026ldquo;Web Interface: Advanced Analysis of Multi-Omics Data.\u0026rdquo; In this article, I share our experience in creating an interactive web solution that facilitates the complex analysis of these datasets.\nWhat are Multi-Omics Data? Before diving into the technical details, let\u0026rsquo;s clarify what multi-omics data are. The suffix \u0026ldquo;-omics\u0026rdquo; refers to the comprehensive study of a specific biological system. Thus:\nTranscriptomics studies the entire set of messenger RNAs (gene expression) Proteomics focuses on the complete set of proteins Metabolomics analyzes all metabolites (small molecules) Phenomics measures the observable characteristics of organisms Each of these information layers is represented by large data matrices. Multi-omic analysis aims to integrate these different matrices to understand the complex relationships between the various levels of biological organization.\nThe Challenge: WallOmics Our project focused on WallOmics data, a dataset from the model plant Arabidopsis thaliana. These data were collected from two different organs (rosettes and stems) of five genetic variants (ecotypes), exposed to two different temperature conditions.\nThe main challenge was to explore and analyze this massive data coherently, using matrix factorization approaches and offering an intuitive web interface.\nOur Approach: OmicsMatrix - The Matrixperience Faced with this challenge, we developed \u0026ldquo;OmicsMatrix: The Matrixperience,\u0026rdquo; an interactive web interface based on R Shiny that integrates several advanced analysis methods. Here are the main features we implemented:\n1. Intelligent Data Loading and Preprocessing The first step was to enable easy data loading and immediate visualization. Our interface offers the ability to load data from WallOmics Data and automatically prepare it for analysis.\nOur preprocessing pipeline automatically handles:\nDetection and treatment of missing values Data normalization Optional exclusion of irrelevant variables 2. In-depth Visual Exploration Visual exploration is crucial for understanding data structure before applying more complex methods. Our \u0026ldquo;Exploration \u0026amp; PCA\u0026rdquo; panel offers:\nA global summary of datasets Visualizations of variable distributions Interactive correlation matrices Principal Component Analysis (PCA) for dimensionality reduction PCA proved particularly useful for identifying the most important variables that explain data variance and for visualizing sample separation in a reduced space.\n3. Advanced Data Integration Methods Our interface offers four main methods of multi-omic analysis, each with specific advantages:\nRegularized Canonical Correlation Analysis (rCCA) This method allows for exploring correlations between two omics datasets. In our experience, rCCA was particularly effective in discovering relationships between transcriptomic and proteomic data.\nOne of the challenges encountered with rCCA was the choice of regularization parameters. We implemented two approaches:\nCross-validation (resource-intensive but accurate) Shrinkage approach (faster but less optimal) Partial Least Squares (PLS) The PLS method allowed us to maximize covariance between different data matrices. Its main advantage is its ability to handle highly correlated data, a common characteristic of omics data.\nWe also implemented the sparse version (Sparse PLS) which automatically selects the most important variables, thus reducing model complexity.\nPLS-DA for Classification For classification questions (e.g., distinguishing samples according to their ecotype or growth condition), we used the PLS-DA (Partial Least Squares-Discriminant Analysis) method.\nThis method proved particularly useful for identifying biological markers that discriminate between different experimental conditions.\nDIABLO for Multiple Integration To simultaneously integrate more than two omics datasets, we implemented DIABLO (Data Integration Analysis for Biomarker Discovery using Latent variable approaches for Omics studies).\nDIABLO was the most powerful method in our arsenal, allowing us to discover biomarkers associated with the studied phenotypes by combining information from all available omic layers.\nKey Strategies for Effective Multi-Omics Analysis Based on extensive experience with the OmicsMatrix platform, four critical strategies emerge for successful multi-omics data integration:\n1. Rigorous Exploratory Analysis as Foundation Comprehensive exploratory analysis must precede any advanced analytical techniques. This foundational step reveals data structure, identifies outliers, and guides subsequent methodological choices. Investing time in thorough exploration consistently yields more interpretable and biologically meaningful final results.\n2. Interactive Analysis Workflows Multi-omics analysis demands an iterative approach with continuous parameter refinement. Real-time visualization of these adjustments\u0026rsquo; impact is essential for optimal results. The R Shiny framework provides the necessary reactive environment to support this advanced analytical process.\n3. Method Selection Driven by Biological Questions Each analytical method serves distinct biological objectives. For general data structure understanding, PCA excels; for pairwise dataset relationships, rCCA provides optimal insights; for prediction models, standard PLS offers robust solutions; for classification tasks, PLS-DA delivers superior performance; while comprehensive integration across multiple omics layers requires DIABLO\u0026rsquo;s sophisticated approach.\n4. Advanced Visualization for Biological Interpretation Even the most sophisticated analytical results remain ineffective without appropriate visualization techniques. Strategic data visualization transforms complex statistical outputs into interpretable biological insights, facilitating both analysis and communication of findings to diverse stakeholders.\nConclusion The analysis of multi-omics data represents a considerable challenge in bioinformatics, but our experience with OmicsMatrix shows that a well-designed web interface can greatly facilitate this process.\nOur solution has made it possible to effectively analyze WallOmics data and draw relevant biological conclusions about Arabidopsis thaliana\u0026rsquo;s response to different environmental conditions.\nThe explosion of high-throughput biological data continues to transform biological research, and tools like OmicsMatrix serve as critical bridges between raw data complexity and actionable biological knowledge.\nThis article is based on a project carried out in collaboration with Zine-Eddine F, Mohammed I A, and Lounes M, under the supervision of Lazhar L, as part of the MLSD Master\u0026rsquo;s program at UFR Biom√©dical.\n","date":"2023-05-05T00:00:00Z","image":"https://zinef.github.io/p/multiomics/dna-closely_hu_99af59f10c38fdf6.jpg","permalink":"https://zinef.github.io/p/multiomics/","title":"How to Effectively Explore and Analyze Multi-Omics Data: Experience Report on Our Web Interface"}]